---
author: "YOUR NAME HERE"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

## Week 6 homework - Transfer function rainfall-runoff model  

In this lab, you will build a simple lumped rainfall–runoff model from the ground up. The structure is based on the TRANSEP model (Weiler et al., 2003), but we will only use the core hydrologic components:

-- the loss function (after Jakeman and Hornberger), and

-- the gamma transfer function for routing water through the watershed.

We are intentionally leaving out the tracer module so that we can focus on how rainfall is transformed into streamflow.
The data for this model come from the Tenderfoot Creek Experimental Forest in central Montana.

Our overall objective is to simulate runoff and compare observed versus modeled discharge. But the deeper goal is not simply to “make the model run.” It is to understand how each modeling decision shapes system behavior. Over the course of this script, you will:

-- Define model parameters.

-- Implement the loss function.

-- Define the runoff transfer function.

-- Convolve rainfall with the transfer function to generate simulated discharge.

-- Compare simulated and observed hydrographs.

Each step builds directly on the physical assumptions we discussed in the bookdown book. By writing each component yourself, you will see how parameter choices control the timing, magnitude, and shape of runoff response.

This matters for two reasons.

First, environmental models are not black boxes. Every equation reflects an assumption about how the system works. We need to understand those assumptions before we can interpret results.

Second, in the next unit we will use Monte Carlo simulations to explore parameter uncertainty. Before we let an algorithm suggest parameter values and sensitivity, you need intuition for how parameters influence model behavior. 

Therefore, the goal is not to jump through coding steps. The goal is to understand how rainfall becomes runoff in a model and how your choices control that transformation.

The usual setup chunk using the package testing function:
```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = TRUE, comment = TRUE, message = FALSE, error = TRUE)

# Write your package testing function
pkgTest <- function(x)
{
  if (x %in% rownames(installed.packages()) == FALSE) {
    install.packages(x, dependencies= TRUE)
  }
  library(x, character.only = TRUE)
}

# Make a vector of the packages you need
neededPackages <- c('tidyverse', 'tictoc', 'patchwork') 

# For every package in the vector, apply your pkgTest function
for (package in neededPackages){pkgTest(package)}

# tictoc times the execution of different modeling methods
# patchwork is used for side-by-side plots
```

#### Read in the data
Let's try a new trick along the way. Assume that you know you put your data file in the working directory, but cannot recall its name. Let's do some working directory exploration with script:

```{r}
# Let's try a new trick along the way. Assume that you know you put your data file in the working directory, but cannot recall its name. Let's do some working directory exploration with script:

# Check your working directory:


# Check the datafile name or path by listing files in the working directory.
filepaths <-
  
# Here is an option to list only .csv files in your working directory:
csv_files <- 
  
print(csv_files)
```

Now we'll read in the data and do some transformation. This will look familiar.
```{r}
# Identify the path to the desired data. 
filepath <- 
indata <- 

indata <-
 # convert "Date" to a date object with mdy()
 # add the water year
```

#### Define input year
We could use every year in the time series, but for starters, we'll use 2006. Use filter() to extract the 2006 water year.
```{r define_year}
PQ <-  # extract the 2006 water year with filter()

# plot discharge for 2006

```

#### Define the initial inputs
Before we simulate anything, we need to define the basic time series the model will use.

In this step, we extract:
-- a simple timestep index to keep track of model time.
-- observed precipitation (model input),
-- observed discharge (for later comparison), and

At this stage, we are not modeling anything yet. We are just defining the inputs and reference output that will anchor the rest of the workflow.
```{r define_inputs}
tau <-  # simple timestep counter the same length as PQ
Pobs <-  # observed precipitation from PQ df
Qobs <-  # observed streamflow from PQ df
```

#### Parameterization 

Now we define the model parameters. These values control how precipitation is partitioned, stored, and routed through the watershed.

You are welcome to experiment with them later but first, make sure the model runs correctly with these baseline values. Even small parameter changes can substantially alter the magnitude, timing, and shape of simulated runoff.
```{r define_parameters}
# Loss function parameters
b1 <- 0.0018 # volume control parameter (b1 in eq 4a)
b2 <- 50 # backwards weighting parameter (b2 in eq 4a) 
b3 <- 0.135 # initial s(t) value for s(t=0) (b3 in eq 4b) - Initial antecedent precipitation index value.

# Transfer function parameters
a <- 1.84 # TF shape parameter
b <- 3.29 # TF scale parameter
```

##### Why parameters matter

These parameters are not just tuning knobs; they represent assumptions about storage, memory, and routing within the watershed. As you adjust them, watch for:

-- Changes in peak magnitude
-- Shifts in timing
-- Broadening or sharpening of the hydrograph

#### Loss function
The loss function determines how much of the observed precipitation actually becomes effective precipitation (p_eff), i.e., the portion of rainfall that contributes to runoff.

Not all rainfall immediately becomes streamflow. Some water is stored, some infiltrates, and some evaporates. The loss function represents this filtering process.

In the Jakeman and Hornberger formulation, effective precipitation depends on two things:
-- the current precipitation input, and
-- the watershed’s antecedent wetness state.

We compute this in three steps:
1) Preallocate p_eff

We first create an empty vector to store effective precipitation at each timestep.
Recall from the book: P_eff(t)=P(t)s(t)

Here:
P(t) is observed precipitation, and
s(t) represents watershed wetness.

Effective precipitation is the portion of rainfall that generates streamflow. Conceptually, it represents event water and the displacement of pre-event water into the stream.
```{r loss_function1}
# preallocate the p_eff vector (length())
p_eff <- 
```

2) Initialize the antecedent index, s(t)

The variable s(t) is an antecedent precipitation index (API). It tracks how wet the watershed is over time. If the catchment is already wet, a larger fraction of rainfall becomes runoff. If the catchment is dry, more rainfall is “lost” before contributing to streamflow.

The parameter b2 controls how strongly past rainfall influences current wetness. It acts as a memory parameter:
-- Larger b2 → longer memory (past rainfall influences the present more strongly)
--Smaller b2 → shorter memory (system responds mainly to recent rainfall)

Mathematically, s(t) is updated by exponentially weighting past precipitation backward in time. Parameter b2 is a 'dial' that places weight on past precipitation events. 

3) Compute p_eff in a loop

At each timestep, we:
-- Update the antecedent index s(t)
-- Multiply it by current precipitation
-- Store the resulting effective precipitation

This produces a time series of water that is available for routing through the watershed.

Note:
Weiler et al. (2003) suggest that the volume control parameter can be determined directly from measured input. In practice, this is not the case, it must be calibrated.

```{r loss_function2_3}
s <- # at this point, s is equal to b3 (see previous chunk)

# Because the antecedent index depends on its previous value, we must update it sequentially through time. This is why a loop is appropriate here.
# loop with loss function
for (i in 1:length(p_eff)) {
  s <- # this is eq 4a from Weiler et al. (2003)
  p_eff[i] <-  # this is eq 4c from Weiler et al. (2003)
}

```

Plot observed and effective precipitation against the date
```{r loss_function2_3}
# wide to long with pivot_longer()
precip <- 

# Plot P time series is with a step function. ggplot2 has geom_step() for this.
p1 <- 

## plot the ratio of p_eff/Pobs (call that ratio "frac")
precip_new <- tibble(date = PQ$Date, obs = Pobs, sim = p_eff, frac = p_eff/Pobs)

p2 <- 

p1 / p2   # stacks vertically 
```

**2) QUESTION (3 pts): Interpret the two figures. What is the meaning of "frac"? For this answer, think about what the effective precipitation represents. When is frac "high", when is "frac" low?** 

ANSWER: 

**Extra Credit (2 pt): Combine the two plots into one, with a meaningfully scaled secondary y-axis**  

#### Runoff transfer function
Ok, up to this point, we have determined *how much* precipitation becomes effective rainfall (p_eff).

Now we answer a different question:
Once water is available for runoff, *how is it distributed* through time as it moves through the watershed?

In other words, the loss function determines how much water enters the routing system.
The transfer function determines when that water exits.

The transfer function represents the travel time distribution of water. It describes how a pulse of effective precipitation is spread out before it reaches the stream outlet.
In this implementation, we use a gamma distribution to represent that travel time behavior.
This section contains two steps:
-- Compute the (raw) gamma transfer function.
-- Normalize it so that the total area under the curve equals 1.

tau represents time since a rainfall pulse.
The mean residence time of the distribution is controlled by the shape (a) and scale (b) parameters.
```{r routing_function}
# raw gamma transfer function (Eq. 13, Weiler et al. 2003 — corrected form)
g <- 
# normalize the TF, g, by the total sum of g.
gTF <- 

# plot the transfer function as a line against tau. You need to put the two vectors tau and gTF into a new df/tibble for this. 
tf_plot <- 


```
Think of this curve as the watershed’s “fingerprint” for routing water.

**3) QUESTION (2 pt): Why must the transfer function sum to 1 before we use it in convolution?**  

ANSWER: 

**4) QUESTION (2 pt): Does the transfer function represent discharge after normalization? Why or why not?** 

ANSWER:

**5) (2 pt)Look at the shape of the transfer function. If the peak occurs very early in time, what does that suggest about how quickly water moves through the watershed? If the curve is broad and spread out, what does that suggest about storage and travel times?**

ANSWER: 


#### Convolution

This is the heart of the model. 
Up to now we have:

p_eff → how much water is available
gTF → when that water arrives

Convolution combines these two to generate simulated runoff. So conceptually, each pulse of effective precipitation is distributed forward in time according to the transfer function.

Why timesteps matter

We are modeling one water year (365 days).
However, our transfer function is also 365 timesteps long. This means that rainfall occurring late in the year will continue contributing runoff into the following year. In other words:
A rainfall pulse on day 300 will still be producing runoff on day 350, 360, and potentially beyond day 365.
So we must decide how to handle runoff that extends beyond our modeling window.

There are a couple of ways to do this, we'll stick with the option that is easiest to code. 
-- Compute the full convolution matrix.
-- Sum contributions.
-- Trim the result to 365 days at the end.
What do you think this will do the cumulative sum of effective precipitation if compared to the observed precip?

What Convolution Is Doing

For each timestep of p_eff, a mini hydrograph is defined by the transfer function. 
If effective precipitation occurs at time step t: 
We multiply p_eff[t] by the entire transfer function.
That produces runoff contributions at time t, t+1, t+2, … etc.
Each loop iteration produces one row of contributions.
If we stack all of those rows in a matrix:

Row 1 → contribution of precipitation at timestep 1

Row 2 → contribution of precipitation at timestep 2

Row 3 → contribution of precipitation at timestep 3…

When we sum across rows, we get:
q_all[t] = total simulated runoff at timestep t.

```{r convolution}
# preallocate qsim matrix with the correct dimensions. Remember that p_eff and gTF are the same length.
q_all_loop <- matrix() # set number of rows and columns

# convolution for-loop
for (i in 1:length(p_eff)) { # loop through length of precipitation timeseries
  # populate the q_all matrix (this is the same code as the UH convolution problem with one tiny change because you need to reference gTF and not UH)
}
# view(q_all_loop)
```
Check out q_all_loop:
Each column = contribution from one rainfall timestep
Each row = runoff at a specific time
Matrix is oversized to allow spillover beyond day 365

```{r convolution}
# add up the rows of the matrix to generate the final runoff and replace matrix with final Q
q_all <- 
```
Summing across rows combines all individual mini-hydrographs into one final simulated hydrograph

Now lets handle edge effects:
```{r convolution}
# cut the vector to the appropriate length of one year (otherwise it won't fit into the original df with the observed data)
q_all <- 

# Write the final runoff vector into the PQ df
PQ$Qsim <- 
```
**5) QUESTION (3 pts): By setting the transfer function length to 365 days, what assumption are we making about maximum travel times in the watershed? Would a shorter or longer transfer function change model behavior? Why?**  

ANSWER: 


### Visualizing the Convolution

Before we collapse the matrix into one hydrograph, let’s look at the individual contributions.

Each column of q_all_loop represents the runoff generated by one timestep of effective precipitation distributed forward in time.

By plotting all of them together, we can see how convolution builds the final hydrograph.

```{r}
## THIS PART SAVES ALL HOURLY Q RESPONSES IN A NEW DF AND PLOTS THEM
Qall <- 


# Qall[Qall == 0] <- NA

Qall <- 
  # add the time vector
  gather(key, value, -Time_hrs)

Qall$key <- as.factor(Qall$key)

ggplot(Qall
```
Each faint line represents the runoff response to a single timestep of effective precipitation.

Notice how:
-- Each pulse is spread forward in time according to the transfer function.
-- Later rainfall pulses are shifted forward relative to earlier ones.
--The final hydrograph (which we compute by summing these) is simply the accumulation of all these responses.

#### Plot
Plot the observed and simulated runoff. Include a legend and label the y-axis.
```{r plot}
# make long form
PQ_long <- PQ %>%
  select(-wtr_yr, -RainMelt_mm) %>%
  pivot_longer(names_to = "key", values_to = "value", -Date)

# plot hydrographs (as lines) and label the y-axis
ggplot()
```
We have now built the full rainfall–runoff model:

-- Precipitation → filtered by the loss function
-- Effective precipitation → routed using the transfer function
-- Convolution → generates simulated discharge

Now we compare the simulated hydrograph with observed discharge to evaluate model performance.

**6) QUESTION (3 pt): Visually evaluate model performance. Does the model capture peak timing? Does it over- or under-estimate peak magnitude? How well does it represent low flows?**  

ANSWER:

```{r}
sum(Pobs) # observed P

# effective (modeled) P

# modeled Q

 # observed Q
```
**7) QUESTION (2 pt): Compare the total effective precipitation with the total simulated runoff and the total observed runoff. What does p_eff physically represent? Why should ∑p_eff and ∑q_all be nearly equal? Why is there a (small) mismatch between the sums of p_eff and q_all?**    
ANSWER: 


Exploring Parameter Sensitivity

Before moving on, save the original parameter values provided above. These will serve as your baseline configuration.

Now, experiment. Change one parameter at a time while keeping all others fixed. After each change:
-- Re-run the model.
-- Re-plot the simulated hydrograph.
-- Observe what changed.

As you experiment, ask yourself:
--Does this parameter primarily affect peak magnitude?
--Does it shift timing?
--Does it change recession behavior?
--Does it affect total runoff volume?

Avoid changing multiple parameters at once; that makes it impossible to isolate effects.

The goal here is not to “improve” the model yet.
The goal is to build intuition about how each parameter controls system behavior.

**8) QUESTION (2pt) Select two model parameters and adjust them (one at a time) away from their baseline values. For each parameter:**
**State the original value. State the new value you tested.**
**Briefly describe how the simulated hydrograph changed (e.g., peak magnitude, timing, recession, total runoff).**

After completing this exploration, return all parameters to their original values before submitting your script.

In the next unit, we will use Monte Carlo simulations to systematically explore parameter space.


